{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timmyt110/Deep-Q-Learning-on-Atari-Final-Project/blob/main/Mario_DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S98RFbrxjnjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69e73f2-8bd8-4307-9d40-d2e27acca2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# NEW BASE PATH for COLAB (change folder name if you want)\n",
        "base = \"/content/drive/MyDrive/CSCI166_RL_New3\"\n",
        "\n",
        "paths = {\n",
        "    \"models\":  f\"{base}/Models\",\n",
        "    \"results\": f\"{base}/Results\",\n",
        "    \"videos\":  f\"{base}/Videos\",\n",
        "    \"runs\":    f\"{base}/Runs\"\n",
        "}\n",
        "\n",
        "# Create directories\n",
        "for p in paths.values():\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "paths\n"
      ],
      "metadata": {
        "id": "4Kyv9_iikfL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c4e45e-d51b-4b70-f516-3ff9d5431652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'models': '/content/drive/MyDrive/CSCI166_RL_New3/Models',\n",
              " 'results': '/content/drive/MyDrive/CSCI166_RL_New3/Results',\n",
              " 'videos': '/content/drive/MyDrive/CSCI166_RL_New3/Videos',\n",
              " 'runs': '/content/drive/MyDrive/CSCI166_RL_New3/Runs'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"gymnasium[atari,accept-rom-license]\"\n",
        "!pip install -q autorom\n",
        "!AutoROM --accept-license\n"
      ],
      "metadata": {
        "id": "C1jBqD2Wkxd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ace4980-23b9-4abf-cc1e-fbdd97fbe1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mAutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.12/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 6: DQN model ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        # input_shape is (C, H, W)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # figure out the size of the conv output\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, *input_shape)\n",
        "            conv_out_size = self.conv(dummy).shape[-1]\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is uint8 images in [0, 255]\n",
        "        x = x.float() / 255.0\n",
        "        return self.fc(self.conv(x))\n"
      ],
      "metadata": {
        "id": "3D0umnMmldmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 7: Replay buffer ===\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "Experience = collections.namedtuple(\n",
        "    \"Experience\",\n",
        "    [\"state\", \"action\", \"reward\", \"done\", \"next_state\"],\n",
        ")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buf = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buf)\n",
        "\n",
        "    def push(self, e: Experience):\n",
        "        # e should be an Experience(...)\n",
        "        self.buf.append(e)\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        batch = random.sample(self.buf, batch_size)\n",
        "        s, a, r, d, sp = zip(*batch)\n",
        "\n",
        "        return (\n",
        "            np.stack(s, axis=0),            # (B, C, H, W) uint8\n",
        "            np.array(a, dtype=np.int64),    # (B,)\n",
        "            np.array(r, dtype=np.float32),  # (B,)\n",
        "            np.array(d, dtype=np.uint8),    # (B,)\n",
        "            np.stack(sp, axis=0),           # (B, C, H, W) uint8\n",
        "        )\n"
      ],
      "metadata": {
        "id": "dDqWP141lzTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 8: Helpers ===\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def epsilon_by_frame(step: int, eps_start: float, eps_end: float, eps_decay_steps: int) -> float:\n",
        "    frac = max(0.0, 1.0 - step / float(eps_decay_steps))\n",
        "    return eps_end + (eps_start - eps_end) * frac\n",
        "\n",
        "def obs_to_chw(obs) -> np.ndarray:\n",
        "    arr = np.array(obs)            # (H, W, C)\n",
        "    return np.transpose(arr, (2, 0, 1))  # (C, H, W)\n",
        "\n",
        "def select_action(online_net, state_chw: np.ndarray, epsilon: float, action_space, device: str):\n",
        "    # epsilon-greedy exploration\n",
        "    if random.random() < epsilon:\n",
        "        return action_space.sample()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        s = torch.from_numpy(state_chw[None, ...]).to(device)  # (1, C, H, W)\n",
        "        q = online_net(s)\n",
        "        return int(q.argmax(dim=1).item())\n"
      ],
      "metadata": {
        "id": "sIEv3mBAl0Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 9: DDQN update ===\n",
        "# === Step 9: Environment Setup (AtariPreprocessing + FrameStack) ===\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Use this import path for AtariPreprocessing (works across versions)\n",
        "try:\n",
        "    from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
        "except ImportError:\n",
        "    from gymnasium.wrappers.atari import AtariPreprocessing\n",
        "\n",
        "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
        "\n",
        "class SimpleFrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k=4):\n",
        "        super().__init__(env)\n",
        "        self.k = k\n",
        "        self.frames = deque(maxlen=k)\n",
        "        h, w = 84, 84\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(h, w, k), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.frames.clear()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(obs)\n",
        "        return np.stack(self.frames, axis=-1), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, term, trunc, info = self.env.step(action)\n",
        "        self.frames.append(obs)\n",
        "        return np.stack(self.frames, axis=-1), reward, term, trunc, info\n",
        "\n",
        "\n",
        "def make_env(seed: int = 0):\n",
        "    env = gym.make(\n",
        "        \"ALE/MarioBros-v5\",\n",
        "        frameskip=1,\n",
        "        repeat_action_probability=0.25,\n",
        "        full_action_space=True,\n",
        "    )\n",
        "\n",
        "    env = AtariPreprocessing(\n",
        "        env,\n",
        "        frame_skip=4,\n",
        "        screen_size=84,\n",
        "        grayscale_obs=True,\n",
        "        scale_obs=False,\n",
        "        terminal_on_life_loss=False,\n",
        "    )\n",
        "\n",
        "    env = SimpleFrameStack(env, k=4)\n",
        "    env = RecordEpisodeStatistics(env)\n",
        "\n",
        "    # This is to record training videos every 50 episodes\n",
        "    env = RecordVideo(\n",
        "        env,\n",
        "        video_folder=paths[\"videos\"],\n",
        "        episode_trigger=lambda ep: ep % 50 == 0,\n",
        "        name_prefix=\"train_mario\",\n",
        "    )\n",
        "\n",
        "    env.action_space.seed(seed)\n",
        "    return env\n",
        "\n"
      ],
      "metadata": {
        "id": "G9pTtC1al74E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Use this import path for AtariPreprocessing (works across versions)\n",
        "try:\n",
        "    from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
        "except ImportError:\n",
        "    # older layout fallback\n",
        "    from gymnasium.wrappers.atari import AtariPreprocessing\n",
        "\n",
        "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
        "\n",
        "# ---- Minimal, version-proof FrameStack replacement ----\n",
        "class SimpleFrameStack(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Stacks the last `k` grayscale frames along the last axis (H, W, k).\n",
        "    Returns uint8 [0,255] like the real FrameStack.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, k=4):\n",
        "        super().__init__(env)\n",
        "        self.k = k\n",
        "        self.frames = deque(maxlen=k)\n",
        "        # After AtariPreprocessing(grayscale_obs=True, screen_size=84) -> (84, 84)\n",
        "        h, w = 84, 84\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(h, w, k), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.frames.clear()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(obs)  # seed stack with first frame\n",
        "        return self._get_obs(), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.frames.append(obs)\n",
        "        return self._get_obs(), reward, terminated, truncated, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # Stack along the last dimension -> (H, W, k)\n",
        "        return np.stack(list(self.frames), axis=-1)\n",
        "\n",
        "def make_env(env_name=\"ALE/MarioBros-v5\", seed=None):\n",
        "    # 1. Create the base environment\n",
        "    #    frameskip=1 + repeat_action_prob is a standard 'v5' / sticky-action setting\n",
        "    env = gym.make(\n",
        "        env_name,\n",
        "        render_mode=\"rgb_array\",\n",
        "        frameskip=1,\n",
        "        repeat_action_probability=0.25,\n",
        "        full_action_space=True\n",
        "    )\n",
        "\n",
        "    # 2. Apply standard Atari preprocessing\n",
        "    env = AtariPreprocessing(\n",
        "        env,\n",
        "        frame_skip=4,\n",
        "        screen_size=84,\n",
        "        grayscale_obs=True,\n",
        "        scale_obs=False,            # returns uint8 [0,255]\n",
        "        terminal_on_life_loss=False\n",
        "    )\n",
        "\n",
        "    # 3. Stack frames\n",
        "    env = SimpleFrameStack(env, k=4)\n",
        "\n",
        "    # 4. Record basic stats (episode return/length) in info\n",
        "    env = RecordEpisodeStatistics(env)\n",
        "\n",
        "    if seed is not None:\n",
        "        env.reset(seed=seed)\n",
        "        env.action_space.seed(seed)\n",
        "\n",
        "    return env"
      ],
      "metadata": {
        "id": "vBetfK3cmTor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 10: Training setup & loop (Mario DDQN) ===\n",
        "import os, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F # <-- Added for mse_loss\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Safe TensorBoard import (falls back to dummy if missing)\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except Exception as e:\n",
        "    print(\" TensorBoard not available, using dummy SummaryWriter:\", e)\n",
        "\n",
        "    class SummaryWriter:\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            pass\n",
        "        def add_scalar(self, *args, **kwargs):\n",
        "            pass\n",
        "        def close(self):\n",
        "            pass\n",
        "\n",
        "# ---- Config ----\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    lr: float = 1e-4\n",
        "    gamma: float = 0.99\n",
        "    batch_size: int = 32\n",
        "    grad_clip: float = 10.0\n",
        "    replay_size: int = 1_000_000\n",
        "    warmup_steps: int = 50_000\n",
        "    update_every: int = 4\n",
        "    target_sync: int = 10_000\n",
        "    eps_start: float = 1.0\n",
        "    eps_end: float = 0.05\n",
        "    eps_decay_steps: int = 1_000_000\n",
        "    max_frames: int = 1_000_000\n",
        "\n",
        "CFG = TrainConfig()\n",
        "\n",
        "# Use the folder structure we already created in Step 1\n",
        "MODELS_DIR  = paths[\"models\"]\n",
        "RESULTS_DIR = paths[\"results\"]\n",
        "RUNS_DIR    = paths[\"runs\"]\n",
        "VIDEOS_DIR  = paths[\"videos\"]  # mostly used by RecordVideo in make_env\n",
        "\n",
        "for d in [MODELS_DIR, RESULTS_DIR, RUNS_DIR, VIDEOS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# NOTE: we are reusing the make_env() and SimpleFrameStack defined earlier,\n",
        "# which already wrap Mario with AtariPreprocessing + FrameStack + RecordVideo.\n",
        "\n",
        "# ---- DDQN Learn Step Function (Missing from notebook) ----\n",
        "def ddqn_learn_step(online_net, target_net, optimizer, buffer, batch_size, gamma, grad_clip, device):\n",
        "    s, a, r, d, sp = buffer.sample(batch_size)\n",
        "\n",
        "    # Convert to tensors\n",
        "    s_t = torch.from_numpy(s).to(device).float() / 255.0  # Normalized\n",
        "    a_t = torch.from_numpy(a).to(device)\n",
        "    r_t = torch.from_numpy(r).to(device)\n",
        "    d_t = torch.from_numpy(d).to(device).float()\n",
        "    sp_t = torch.from_numpy(sp).to(device).float() / 255.0 # Normalized\n",
        "\n",
        "    # Compute Q(s,a) for current states\n",
        "    q_values = online_net(s_t)\n",
        "    # Gather the Q-values for the actions taken\n",
        "    q_s_a = q_values.gather(1, a_t.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # Compute Q'(s',a') for next states using online network (for action selection)\n",
        "    with torch.no_grad():\n",
        "        next_q_values_online = online_net(sp_t)\n",
        "        # Select best action from online network\n",
        "        next_actions = next_q_values_online.argmax(dim=1)\n",
        "        # Compute Q'(s',a') using target network (for value estimation)\n",
        "        next_q_values_target = target_net(sp_t)\n",
        "        q_s_prime_a_prime = next_q_values_target.gather(1, next_actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Compute target Q-values\n",
        "        # (1 - d_t) ensures that if done=True, the next state value is 0\n",
        "        target_q_s_a = r_t + gamma * q_s_prime_a_prime * (1 - d_t)\n",
        "\n",
        "    # Compute loss (MSE between predicted Q and target Q)\n",
        "    loss = F.mse_loss(q_s_a, target_q_s_a)\n",
        "\n",
        "    # Optimize the online network\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # Gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(online_net.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# ---- Trainer (expects DQN, ReplayBuffer, Experience, helpers from Steps 6â€“9) ----\n",
        "def train_ddqn(run_label: str,\n",
        "               env_name: str = \"ALE/MarioBros-v5\",\n",
        "               cfg: TrainConfig = CFG):\n",
        "    # Reuse your existing make_env() (already wired to MarioBros)\n",
        "    env = make_env(seed=0)\n",
        "    obs, _ = env.reset()\n",
        "    action_space = env.action_space\n",
        "\n",
        "    # Env returns (H, W, 4); our network expects (C, H, W)\n",
        "    C, H, W = 4, 84, 84\n",
        "    n_actions = action_space.n\n",
        "\n",
        "    online = DQN((C, H, W), n_actions).to(cfg.device)\n",
        "    target = DQN((C, H, W), n_actions).to(cfg.device)\n",
        "    target.load_state_dict(online.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(online.parameters(), lr=cfg.lr)\n",
        "    buffer = ReplayBuffer(cfg.replay_size)\n",
        "\n",
        "    tb = SummaryWriter(log_dir=os.path.join(RUNS_DIR, run_label))\n",
        "    csv_path = os.path.join(RESULTS_DIR, f\"{run_label}.csv\")\n",
        "    if os.path.exists(csv_path):\n",
        "        os.remove(csv_path)\n",
        "\n",
        "    state_chw = obs_to_chw(obs)\n",
        "    ep_return, ep_idx, step = 0.0, 0, 0\n",
        "\n",
        "    while step < cfg.max_frames:\n",
        "        step += 1\n",
        "        eps = epsilon_by_frame(step, cfg.eps_start, cfg.eps_end, cfg.eps_decay_steps)\n",
        "\n",
        "        action = select_action(online, state_chw, eps, action_space, cfg.device)\n",
        "        next_obs, reward, term, trunc, info = env.step(action)\n",
        "        done = term or trunc\n",
        "        next_chw = obs_to_chw(next_obs)\n",
        "\n",
        "        buffer.push(Experience(state_chw, action, reward, done, next_chw))\n",
        "        ep_return += reward\n",
        "        state_chw = next_chw\n",
        "\n",
        "        if done:\n",
        "            ep_idx += 1\n",
        "            tb.add_scalar(\"charts/episode_return\", ep_return, step)\n",
        "            tb.add_scalar(\"charts/epsilon\", eps, step)\n",
        "            if \"episode\" in info:   # fixed key name\n",
        "                tb.add_scalar(\"charts/episode_length\", info[\"episode\"][\"l\"], step)\n",
        "\n",
        "            with open(csv_path, \"a\") as f:\n",
        "                f.write(f\"{step},{ep_idx},{ep_return}\\n\")\n",
        "\n",
        "            obs, _ = env.reset()\n",
        "            state_chw = obs_to_chw(obs)\n",
        "            ep_return = 0.0\n",
        "\n",
        "        # Fill replay buffer before training\n",
        "        if len(buffer) < cfg.warmup_steps:\n",
        "            continue\n",
        "\n",
        "        # Gradient step\n",
        "        if step % cfg.update_every == 0:\n",
        "            loss = ddqn_learn_step(\n",
        "                online, target, optimizer, buffer,\n",
        "                cfg.batch_size, cfg.gamma, cfg.grad_clip, cfg.device\n",
        "            )\n",
        "            tb.add_scalar(\"loss/td\", loss, step)\n",
        "\n",
        "        # Sync target network\n",
        "        if step % cfg.target_sync == 0:\n",
        "            target.load_state_dict(online.state_dict())\n",
        "\n",
        "        # Periodic checkpoint\n",
        "        if step % 200_000 == 0:\n",
        "            ckpt = os.path.join(MODELS_DIR, f\"{run_label}_{step}.pth\")\n",
        "            torch.save(online.state_dict(), ckpt)\n",
        "            print(f\" saved {ckpt}\")\n",
        "\n",
        "    env.close()\n",
        "    tb.close()\n",
        "    print(f\" Finished {run_label}\")\n",
        "    return online"
      ],
      "metadata": {
        "id": "zpLEC8Y6mi3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enwdFOC6oNG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Plot learning curve\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_returns(run_label: str, window: int = 50):\n",
        "    # use the results directory we created in Step 1\n",
        "    RESULTS_DIR = paths[\"results\"]\n",
        "\n",
        "    csv_path = os.path.join(RESULTS_DIR, f\"{run_label}.csv\")\n",
        "    out_png  = os.path.join(RESULTS_DIR, f\"{run_label}_curve.png\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(\" CSV not found:\", csv_path)\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(csv_path, header=None, names=[\"steps\",\"episode\",\"return\"])\n",
        "    df[\"smooth\"] = df[\"return\"].rolling(window, min_periods=1).mean()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(df[\"steps\"], df[\"smooth\"], label=\"Smoothed Return\")\n",
        "    plt.xlabel(\"Env Steps\")\n",
        "    plt.ylabel(\"Episodic Return\")\n",
        "    plt.title(f\"Learning Curve: {run_label}\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig(out_png, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "    print(\" Saved learning-curve plot:\", out_png)\n"
      ],
      "metadata": {
        "id": "2rvIiQr1nosp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 12: Record short videos (10â€“30s) ===\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "\n",
        "def record_clip(env_name: str,\n",
        "                run_label: str,\n",
        "                net,\n",
        "                ckpt_path: str = None,\n",
        "                eps: float = 0.05,\n",
        "                seconds: int = 20,\n",
        "                device: str = CFG.device):\n",
        "\n",
        "    # Use the videos directory from Step 1\n",
        "    VIDEOS_DIR = paths[\"videos\"]\n",
        "    vdir = os.path.join(VIDEOS_DIR, run_label)\n",
        "    os.makedirs(vdir, exist_ok=True)\n",
        "\n",
        "    # Build env with rgb rendering enabled\n",
        "    base = gym.make(\n",
        "        env_name,\n",
        "        frameskip=1,\n",
        "        repeat_action_probability=0.25,\n",
        "        full_action_space=True,\n",
        "        render_mode=\"rgb_array\",  # required for video\n",
        "    )\n",
        "\n",
        "    env = AtariPreprocessing(\n",
        "        base,\n",
        "        frame_skip=4,\n",
        "        screen_size=84,\n",
        "        grayscale_obs=True,\n",
        "        scale_obs=False,\n",
        "        terminal_on_life_loss=False,\n",
        "    )\n",
        "    env = SimpleFrameStack(env, k=4)\n",
        "    env = RecordEpisodeStatistics(env)\n",
        "\n",
        "    # Wrap RecordVideo LAST so it records actual frames\n",
        "    from gymnasium.wrappers import RecordVideo\n",
        "    env = RecordVideo(\n",
        "        env,\n",
        "        video_folder=vdir,\n",
        "        episode_trigger=lambda e: True,   # record first episode\n",
        "        name_prefix=run_label,\n",
        "    )\n",
        "\n",
        "    # Load checkpoint if provided\n",
        "    if ckpt_path is not None:\n",
        "        net.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "\n",
        "    obs, _ = env.reset(seed=123)\n",
        "    done, t0 = False, time.time()\n",
        "\n",
        "    while not done and (time.time() - t0) < seconds:\n",
        "        chw = obs_to_chw(obs)\n",
        "        action = select_action(net, chw, eps, env.action_space, device)\n",
        "        obs, r, term, trunc, _ = env.step(action)\n",
        "        done = term or trunc\n",
        "\n",
        "    env.close()\n",
        "    print(f\" Saved video(s) to: {vdir}   (look for .mp4 files)\")\n"
      ],
      "metadata": {
        "id": "dPTcC2G6oOA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 13: Train DDQN on Mario Bros + Plot + Record Videos (Colab) ===\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "\n",
        "# Register Atari environments (required for Gymnasium v1.0+)\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# -----------------------------\n",
        "#  Paths (from Step 1: `paths`)\n",
        "# -----------------------------\n",
        "MODELS_DIR  = paths[\"models\"]\n",
        "RESULTS_DIR = paths[\"results\"]\n",
        "VIDEOS_DIR  = paths[\"videos\"]\n",
        "\n",
        "for d in [MODELS_DIR, RESULTS_DIR, VIDEOS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"Models dir :\", MODELS_DIR)\n",
        "print(\"Results dir:\", RESULTS_DIR)\n",
        "print(\"Videos dir :\", VIDEOS_DIR)\n",
        "\n",
        "# -----------------------------\n",
        "#  Config overrides\n",
        "# -----------------------------\n",
        "CFG.device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CFG.max_frames   = 205_000          # just over 200k so we hit the auto-save\n",
        "CFG.warmup_steps = 10_000\n",
        "CFG.target_sync  = 5_000\n",
        "CFG.update_every = 4\n",
        "\n",
        "print(\"Using device:\", CFG.device)\n",
        "\n",
        "mario_label = \"DDQN_MarioBros\"\n",
        "\n",
        "# -----------------------------\n",
        "#  Training\n",
        "# -----------------------------\n",
        "print(\" Training on:\", mario_label)\n",
        "\n",
        "# NOTE: our Step 10 defined:\n",
        "#   def train_ddqn(run_label: str, env_name: str = \"ALE/MarioBros-v5\", cfg: TrainConfig = CFG)\n",
        "# So we call it like this:\n",
        "mario_net = train_ddqn(run_label=mario_label, cfg=CFG)\n",
        "\n",
        "# -----------------------------\n",
        "#  Plot learning curve\n",
        "# -----------------------------\n",
        "print(\" Plotting curveâ€¦\")\n",
        "plot_returns(mario_label)\n",
        "\n",
        "# -----------------------------\n",
        "#  Checkpoints\n",
        "# -----------------------------\n",
        "# Saved automatically in train_ddqn at 200k steps:\n",
        "early_ckpt = os.path.join(MODELS_DIR, f\"{mario_label}_200000.pth\")\n",
        "\n",
        "# Save final model at the end of training\n",
        "final_ckpt = os.path.join(MODELS_DIR, f\"{mario_label}_205000.pth\")\n",
        "torch.save(mario_net.state_dict(), final_ckpt)\n",
        "print(\" Saved final checkpoint:\", final_ckpt)\n",
        "\n",
        "# -----------------------------\n",
        "#  Record videos\n",
        "# -----------------------------\n",
        "# Early (more random / less trained)\n",
        "record_clip(\n",
        "    \"ALE/MarioBros-v5\",\n",
        "    \"Mario_early\",\n",
        "    mario_net,\n",
        "    ckpt_path=early_ckpt,    # 200k model\n",
        "    eps=0.20,\n",
        "    seconds=20\n",
        ")\n",
        "\n",
        "# Learned (later policy at 205k)\n",
        "record_clip(\n",
        "    \"ALE/MarioBros-v5\",\n",
        "    \"Mario_learned\",\n",
        "    mario_net,\n",
        "    ckpt_path=final_ckpt,    # 205k model\n",
        "    eps=0.05,\n",
        "    seconds=20\n",
        ")\n",
        "\n",
        "print(\" Step 13 completed successfully!\")\n",
        "print(\" Curves in  :\", RESULTS_DIR)\n",
        "print(\" Models in  :\", MODELS_DIR)\n",
        "print(\" Videos in  :\", VIDEOS_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJN0RsZYoOsI",
        "outputId": "d7b5643e-491d-4df2-d4dc-73d75cb870d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models dir : /content/drive/MyDrive/CSCI166_RL_New3/Models\n",
            "Results dir: /content/drive/MyDrive/CSCI166_RL_New3/Results\n",
            "Videos dir : /content/drive/MyDrive/CSCI166_RL_New3/Videos\n",
            "Using device: cpu\n",
            " Training on: DDQN_MarioBros\n",
            " saved /content/drive/MyDrive/CSCI166_RL_New3/Models/DDQN_MarioBros_200000.pth\n",
            " Finished DDQN_MarioBros\n",
            " Plotting curveâ€¦\n",
            " Saved learning-curve plot: /content/drive/MyDrive/CSCI166_RL_New3/Results/DDQN_MarioBros_curve.png\n",
            " Saved final checkpoint: /content/drive/MyDrive/CSCI166_RL_New3/Models/DDQN_MarioBros_205000.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/CSCI166_RL_New3/Videos/Mario_early folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved video(s) to: /content/drive/MyDrive/CSCI166_RL_New3/Videos/Mario_early   (look for .mp4 files)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/CSCI166_RL_New3/Videos/Mario_learned folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved video(s) to: /content/drive/MyDrive/CSCI166_RL_New3/Videos/Mario_learned   (look for .mp4 files)\n",
            " Step 13 completed successfully!\n",
            " Curves in  : /content/drive/MyDrive/CSCI166_RL_New3/Results\n",
            " Models in  : /content/drive/MyDrive/CSCI166_RL_New3/Models\n",
            " Videos in  : /content/drive/MyDrive/CSCI166_RL_New3/Videos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 13b: Second training run (305k frames) ===\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "mario_label = \"DDQN_MarioBros_305k\"\n",
        "\n",
        "# New config for longer run\n",
        "CFG.device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CFG.max_frames   = 305_000\n",
        "CFG.warmup_steps = 10_000\n",
        "CFG.target_sync  = 5_000\n",
        "CFG.update_every = 4\n",
        "\n",
        "print(\"Using device:\", CFG.device)\n",
        "print(\" Training on:\", mario_label)\n",
        "\n",
        "mario_net_305k = train_ddqn(run_label=mario_label, cfg=CFG)\n",
        "\n",
        "# Plot curve\n",
        "plot_returns(mario_label)\n",
        "\n",
        "# Checkpoints\n",
        "early_ckpt = os.path.join(paths[\"models\"], f\"{mario_label}_200000.pth\")\n",
        "final_ckpt = os.path.join(paths[\"models\"], f\"{mario_label}_305000.pth\")\n",
        "torch.save(mario_net_305k.state_dict(), final_ckpt)\n",
        "print(\"Saved final checkpoint:\", final_ckpt)\n",
        "\n",
        "# Videos\n",
        "record_clip(\"ALE/MarioBros-v5\", \"305k_early\", mario_net_305k, ckpt_path=early_ckpt, eps=0.20, seconds=20)\n",
        "record_clip(\"ALE/MarioBros-v5\", \"305k_late\", mario_net_305k, ckpt_path=final_ckpt, eps=0.05, seconds=20)\n",
        "\n",
        "print(\" 305k run completed!\")\n"
      ],
      "metadata": {
        "id": "uvfpybpWFApm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa62f74-5321-43a3-f1d3-1d226dafbd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            " Training on: DDQN_MarioBros_305k\n",
            " saved /content/drive/MyDrive/CSCI166_RL_New3/Models/DDQN_MarioBros_305k_200000.pth\n",
            " Finished DDQN_MarioBros_305k\n",
            " Saved learning-curve plot: /content/drive/MyDrive/CSCI166_RL_New3/Results/DDQN_MarioBros_305k_curve.png\n",
            "Saved final checkpoint: /content/drive/MyDrive/CSCI166_RL_New3/Models/DDQN_MarioBros_305k_305000.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/CSCI166_RL_New3/Videos/305k_early folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved video(s) to: /content/drive/MyDrive/CSCI166_RL_New3/Videos/305k_early   (look for .mp4 files)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/CSCI166_RL_New3/Videos/305k_late folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved video(s) to: /content/drive/MyDrive/CSCI166_RL_New3/Videos/305k_late   (look for .mp4 files)\n",
            " 305k run completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn_learn_step(online_net, target_net, optimizer, buffer, batch_size, gamma, grad_clip, device):\n",
        "    \"\"\"\n",
        "    Vanilla DQN TD update:\n",
        "      - uses TARGET net to compute max_a' Q(s', a')\n",
        "      - no Double DQN selection step\n",
        "    \"\"\"\n",
        "    s, a, r, d, sp = buffer.sample(batch_size)\n",
        "\n",
        "    s_t  = torch.from_numpy(s).to(device).float() / 255.0\n",
        "    a_t  = torch.from_numpy(a).to(device)\n",
        "    r_t  = torch.from_numpy(r).to(device)\n",
        "    d_t  = torch.from_numpy(d).to(device).float()\n",
        "    sp_t = torch.from_numpy(sp).to(device).float() / 255.0\n",
        "\n",
        "    # Q(s,a) from online net\n",
        "    q_values = online_net(s_t)\n",
        "    q_s_a = q_values.gather(1, a_t.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Vanilla DQN: max_a' Q_target(s', a')\n",
        "        next_q_values = target_net(sp_t)\n",
        "        max_next_q, _ = next_q_values.max(dim=1)\n",
        "        target_q = r_t + gamma * max_next_q * (1 - d_t)\n",
        "\n",
        "    loss = F.mse_loss(q_s_a, target_q)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(online_net.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    return float(loss.item())\n",
        "\n",
        "\n",
        "def train_dqn(run_label: str,\n",
        "              env_name: str = \"ALE/MarioBros-v5\",\n",
        "              cfg: TrainConfig = CFG):\n",
        "\n",
        "    env = make_env(seed=0)\n",
        "    obs, _ = env.reset()\n",
        "    action_space = env.action_space\n",
        "\n",
        "    C, H, W = 4, 84, 84\n",
        "    n_actions = action_space.n\n",
        "\n",
        "    online = DQN((C, H, W), n_actions).to(cfg.device)\n",
        "    target = DQN((C, H, W), n_actions).to(cfg.device)\n",
        "    target.load_state_dict(online.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(online.parameters(), lr=cfg.lr)\n",
        "    buffer = ReplayBuffer(cfg.replay_size)\n",
        "\n",
        "    tb = SummaryWriter(log_dir=os.path.join(RUNS_DIR, run_label))\n",
        "\n",
        "    csv_path = os.path.join(RESULTS_DIR, f\"{run_label}.csv\")\n",
        "    if os.path.exists(csv_path):\n",
        "        os.remove(csv_path)\n",
        "\n",
        "    state_chw = obs_to_chw(obs)\n",
        "    ep_return = 0.0\n",
        "    ep_idx = 0\n",
        "    step = 0\n",
        "\n",
        "    while step < cfg.max_frames:\n",
        "        step += 1\n",
        "        eps = epsilon_by_frame(step, cfg.eps_start, cfg.eps_end, cfg.eps_decay_steps)\n",
        "\n",
        "        # epsilon-greedy action from ONLINE net\n",
        "        action = select_action(online, state_chw, eps, action_space, cfg.device)\n",
        "        next_obs, reward, term, trunc, info = env.step(action)\n",
        "        done = term or trunc\n",
        "        next_chw = obs_to_chw(next_obs)\n",
        "\n",
        "        buffer.push(Experience(state_chw, action, reward, done, next_chw))\n",
        "        ep_return += reward\n",
        "        state_chw = next_chw\n",
        "\n",
        "        if done:\n",
        "            ep_idx += 1\n",
        "            tb.add_scalar(\"charts/episode_return\", ep_return, step)\n",
        "            tb.add_scalar(\"charts/epsilon\", eps, step)\n",
        "\n",
        "            # ðŸ”¹ log to CSV so plot_returns() works\n",
        "            with open(csv_path, \"a\") as f:\n",
        "                f.write(f\"{step},{ep_idx},{ep_return}\\n\")\n",
        "\n",
        "            obs, _ = env.reset()\n",
        "            state_chw = obs_to_chw(obs)\n",
        "            ep_return = 0.0\n",
        "\n",
        "        # warmup period\n",
        "        if len(buffer) < cfg.warmup_steps:\n",
        "            continue\n",
        "\n",
        "        # Gradient update\n",
        "        if step % cfg.update_every == 0:\n",
        "            loss = dqn_learn_step(\n",
        "                online, target, optimizer, buffer,\n",
        "                cfg.batch_size, cfg.gamma, cfg.grad_clip, cfg.device\n",
        "            )\n",
        "            tb.add_scalar(\"losses/td_loss\", loss, step)\n",
        "\n",
        "        # Sync target network\n",
        "        if step % cfg.target_sync == 0:\n",
        "            target.load_state_dict(online.state_dict())\n",
        "\n",
        "        # Optional checkpoint\n",
        "        if step % 200_000 == 0:\n",
        "            ckpt = os.path.join(MODELS_DIR, f\"{run_label}_{step}.pth\")\n",
        "            torch.save(online.state_dict(), ckpt)\n",
        "            print(f\" Saved checkpoint: {ckpt}\")\n",
        "\n",
        "    env.close()\n",
        "    tb.close()\n",
        "    print(f\" Finished training {run_label}\")\n",
        "    return online\n",
        "\n"
      ],
      "metadata": {
        "id": "PycQC3u-A_OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    lr: float = 1e-4\n",
        "    gamma: float = 0.99\n",
        "    eps_start: float = 1.0\n",
        "    eps_end: float = 0.1\n",
        "    eps_decay_steps: int = 300_000\n",
        "    batch_size: int = 32\n",
        "    replay_size: int = 100_000\n",
        "    warmup_steps: int = 10_000\n",
        "    update_every: int = 4\n",
        "    target_sync: int = 10_000\n",
        "    max_frames: int = 300_000\n",
        "    grad_clip: float = 10.0\n",
        "    device: str = \"cpu\"\n",
        "\n",
        "CFG = TrainConfig()\n"
      ],
      "metadata": {
        "id": "KymyhJJLB5sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Run 1: Baseline DQN (project requirement) ----------\n",
        "dqn_label = \"DQN_MarioBros\"\n",
        "CFG.max_frames   = 200_000   # e.g., 200k frames\n",
        "CFG.warmup_steps = 50_000\n",
        "CFG.target_sync  = 10_000\n",
        "CFG.update_every = 4\n",
        "\n",
        "print(\"Using device:\", CFG.device)\n",
        "print(\" Training baseline:\", dqn_label)\n",
        "mario_dqn = train_dqn(run_label=dqn_label, cfg=CFG)\n",
        "plot_returns(dqn_label)\n",
        "\n",
        "# --------- Run 2: Double DQN (your current run) ----------\n",
        "mario_label = \"DDQN_MarioBros\"\n",
        "CFG.max_frames   = 305_000\n",
        "CFG.warmup_steps = 10_000\n",
        "CFG.target_sync  = 5_000\n",
        "CFG.update_every = 4\n",
        "\n",
        "print(\"Using device:\", CFG.device)\n",
        "print(\" Training DDQN:\", mario_label)\n",
        "mario_net_305k = train_ddqn(run_label=mario_label, cfg=CFG)\n",
        "plot_returns(mario_label)\n",
        "\n"
      ],
      "metadata": {
        "id": "uWzR4RjTCDF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc6d0f3-af79-488a-acca-b947f1bcd5a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            " Training baseline: DQN_MarioBros\n",
            " Saved checkpoint: /content/drive/MyDrive/CSCI166_RL_New3/Models/DQN_MarioBros_200000.pth\n",
            " Finished training DQN_MarioBros\n",
            " Saved learning-curve plot: /content/drive/MyDrive/CSCI166_RL_New3/Results/DQN_MarioBros_curve.png\n",
            "Using device: cpu\n",
            " Training DDQN: DDQN_MarioBros\n",
            " saved /content/drive/MyDrive/CSCI166_RL_New3/Models/DDQN_MarioBros_200000.pth\n",
            " Finished DDQN_MarioBros\n",
            " Saved learning-curve plot: /content/drive/MyDrive/CSCI166_RL_New3/Results/DDQN_MarioBros_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Record videos for Baseline DQN -----\n",
        "\n",
        "# CHOOSE a checkpoint (train_dqn saves one at 200k automatically)\n",
        "dqn_ckpt_200k = os.path.join(MODELS_DIR, \"DQN_MarioBros_200000.pth\")\n",
        "\n",
        "# If no checkpoint was saved, save manually:\n",
        "dqn_final_ckpt = os.path.join(MODELS_DIR, \"DQN_MarioBros_final.pth\")\n",
        "torch.save(mario_dqn.state_dict(), dqn_final_ckpt)\n",
        "\n",
        "# ---------- Early-ish DQN behavior ----------\n",
        "record_clip(\n",
        "    \"ALE/MarioBros-v5\",\n",
        "    \"DQN_early\",\n",
        "    mario_dqn,\n",
        "    ckpt_path=dqn_ckpt_200k,\n",
        "    eps=0.25,\n",
        "    seconds=20\n",
        ")\n",
        "\n",
        "\n",
        "# ---------- Learned DQN behavior ----------\n",
        "record_clip(\n",
        "    \"ALE/MarioBros-v5\",\n",
        "    \"DQN_learned\",\n",
        "    mario_dqn,\n",
        "    ckpt_path=dqn_final_ckpt,\n",
        "    eps=0.05,\n",
        "    seconds=20\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uQo_5Xu1_SS",
        "outputId": "49b82d32-99cc-4626-afe8-62531276d259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/CSCI166_RL_New3/Videos/DQN_early folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved video(s) to: /content/drive/MyDrive/CSCI166_RL_New3/Videos/DQN_early   (look for .mp4 files)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/CSCI166_RL_New3/Videos/DQN_learned folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved video(s) to: /content/drive/MyDrive/CSCI166_RL_New3/Videos/DQN_learned   (look for .mp4 files)\n"
          ]
        }
      ]
    }
  ]
}